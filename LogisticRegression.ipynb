{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.use_jedi=False\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Utils import print_memory_usage, frequency_encode_stations, evaluate_model, train, validate, load_data \n",
    "\n",
    "path_train = \"Data/Train.parquet\"\n",
    "path_val = \"Data/Validation.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model I want to try is a simple logistic regression model.\n",
    "Lets start with a simple test model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Variable Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One question here is how to choose a proper encoding for some of the variables.\n",
    "\n",
    "1. Hour of the day: One approach is convert it into a cyclic variable, and another is to use fixed \"binned\" time intervales like morning, midday, evening. We could also convert each hour into a category, but this does not make much sense.\n",
    "2. Numerical Features should be scaled to be comparable? -> Only relevant for gradient optimization, but might want to scale to 0 mean? Also affects regularization, which is on by default -> SHOULD STANDARDIZE, YES (also mentioned in elements of statistical learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_coefficients(model,X):\n",
    "    coefficients = pd.Series([model.intercept_[0]]+list(model.coef_[0]),index=[\"intercept\"]+list(X.columns))\n",
    "    print(\"Coefficients: \")\n",
    "    print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we cannot encode the station id as a one-hot vector because it takes too much memory.\n",
    "Instead I will just encode whether the station is one of the top customer stations or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_stations(data,k=20):\n",
    "    grouped = data_train.groupby(\"usertype\")[\"start station id\"].value_counts()\n",
    "    return grouped[\"Customer\"][0:k].index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Proper data preparation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Take log of some features that span multiple orders of magnitude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hour_to_coordinate(data,features):\n",
    "    xh = np.sin(2*np.pi*(h)/24)\n",
    "    yh = np.cos(2*np.pi*(h)/24)\n",
    "    data[\"xh\"] = xh\n",
    "    data[\"yh\"] = yh\n",
    "    features = features + [\"xh\",\"yh\"]\n",
    "    return data,features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_tod(data,features,add_interactions=False):\n",
    "    #Categories based on plots in Analysis Notebook\n",
    "    hours = data[\"starttime\"].dt.hour \n",
    "    bins=[-1,6,10,15,24] \n",
    "    names=[0,1,2,3]\n",
    "    tod = pd.cut(hours,bins,labels=names)\n",
    "    tod = pd.get_dummies(tod,prefix=\"tod\",drop_first=True)\n",
    "    new=[tod]\n",
    "    features = features + list(tod.columns)\n",
    "    if add_interactions:\n",
    "        interaction = tod.mul(data[\"business day\"],axis=0)\n",
    "        interaction.columns = [\"business x \" + c for c in tod.columns]\n",
    "        new += [interaction]\n",
    "        features = features + list(interaction.columns)\n",
    "    data = data.join(new) \n",
    "    return data,features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try logistic regression with different features and preprocessings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_1(data,features):\n",
    "    \"\"\"\n",
    "    Note that this modifies data inplace\n",
    "    \"\"\"\n",
    "    features = features + [\"hour\"]\n",
    "    data[\"hour\"] = data[\"starttime\"].dt.hour\n",
    "    return data,features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: \n",
      "Accuracy: 0.8941802335692596\n",
      "Confusion: \n",
      "[[0.98174913 0.01825087]\n",
      " [0.8183088  0.1816912 ]]\n",
      "MCC: 0.27340166228644086\n",
      "Validation: \n",
      "Accuracy: 0.8942307307252905\n",
      "Confusion: \n",
      "[[0.98190434 0.01809566]\n",
      " [0.81954388 0.18045612]]\n",
      "MCC: 0.27256028194374954\n"
     ]
    }
   ],
   "source": [
    "pre = preprocess_1\n",
    "features=[\"tripduration\", \"summer\",\"business day\", \"haversine distance\", \"is_roundtrip\", \"speed\"]\n",
    "scaler = MinMaxScaler()\n",
    "features_to_scale = [\"tripduration\",\"haversine distance\", \"speed\"]\n",
    "clf = LogisticRegression(max_iter=100)\n",
    "print(\"Training: \")\n",
    "clf,feature_names = train(path_train,clf,features,preprocess=pre,scaler=scaler,features_to_scale=features_to_scale,fit_scaler = True)\n",
    "print(\"Validation: \")\n",
    "validate(clf,path_val,features,preprocess=pre,scaler=scaler,features_to_scale=features_to_scale,fit_scaler=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorizing tod instead slightly improves the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_2(data,features):\n",
    "    data, features = categorize_tod(data,features)\n",
    "    return data,features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: \n",
      "Accuracy: 0.895168003048048\n",
      "Confusion: \n",
      "[[0.98156348 0.01843652]\n",
      " [0.8077737  0.1922263 ]]\n",
      "MCC: 0.28574764216388404\n",
      "Validation: \n",
      "Accuracy: 0.8951586467528946\n",
      "Confusion: \n",
      "[[0.98161271 0.01838729]\n",
      " [0.80868733 0.19131267]]\n",
      "MCC: 0.2848225615211647\n"
     ]
    }
   ],
   "source": [
    "pre = preprocess_2\n",
    "features=[\"tripduration\", \"summer\",\"business day\", \"haversine distance\", \"is_roundtrip\", \"speed\"]\n",
    "scaler = MinMaxScaler()\n",
    "features_to_scale = [\"tripduration\",\"haversine distance\", \"speed\"]\n",
    "clf = LogisticRegression(max_iter=100)\n",
    "print(\"Training: \")\n",
    "clf,feature_names = train(path_train,clf,features,preprocess=pre,scaler=scaler,features_to_scale=features_to_scale,fit_scaler = True)\n",
    "print(\"Validation: \")\n",
    "validate(clf,path_val,features,preprocess=pre,scaler=scaler,features_to_scale=features_to_scale,fit_scaler=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Add stations, add tod-business day interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_3(data,features):\n",
    "    data, features = categorize_tod(data,features,add_interactions=True)\n",
    "    return data,features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: \n",
      "Accuracy: 0.8953737377031173\n",
      "Confusion: \n",
      "[[0.98178976 0.01821024]\n",
      " [0.80773515 0.19226485]]\n",
      "MCC: 0.28691034219889633\n",
      "Validation: \n",
      "Accuracy: 0.8953239782769251\n",
      "Confusion: \n",
      "[[0.98179514 0.01820486]\n",
      " [0.80866118 0.19133882]]\n",
      "MCC: 0.2857544379373309\n"
     ]
    }
   ],
   "source": [
    "pre = preprocess_3\n",
    "features=[\"tripduration\", \"summer\",\"business day\", \"haversine distance\", \"is_roundtrip\", \"speed\"]\n",
    "scaler = MinMaxScaler()\n",
    "features_to_scale = [\"tripduration\",\"haversine distance\", \"speed\"]\n",
    "clf = LogisticRegression(max_iter=100)\n",
    "print(\"Training: \")\n",
    "clf,feature_names = train(path_train,clf,features,preprocess=pre,scaler=scaler,features_to_scale=features_to_scale,fit_scaler = True)\n",
    "print(\"Validation: \")\n",
    "validate(clf,path_val,features,preprocess=pre,scaler=scaler,features_to_scale=features_to_scale,fit_scaler=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: By using gender=unknown, gender=male categorical features one can easily get about 94% training accuracy, and about 98% on subscribers. Using classes = balanced gives 79% accuracy overall, but like 82% on customers instead of 20% for unbalanced.\n",
    "\n",
    "Scaling data with min-max scaler seems to have no effect\n",
    "Using categorical tod encoding does not seem much different from using ordinal encoding or hours, at least if we dont use interactions.\n",
    "Adding interaction terms between tod and business day does not help much.\n",
    "\n",
    "Adding top_20_customer_start_station as label increases accuracy from about 89.5 % to 90%.\n",
    "Instead using customer counts of each start_station as a feature gives 90% training but only 89.5% validation accuracy\n",
    "\n",
    "Frequency encoding bot start and end station by both subscriber and customer also gives around 90% accuracy.\n",
    "\n",
    "Adding gender to this again gives about 94% accuracy, and 54% on customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further ideas:\n",
    "1. add interaction term between start and end station\n",
    "2. different categories for tod and summer\n",
    "3. Train with class weights=balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
