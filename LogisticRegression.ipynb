{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0571d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.use_jedi=False\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import sklearn.preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Utils import print_memory_usage\n",
    "\n",
    "path_train = \"Data/Train.parquet\"\n",
    "path_val = \"Data/Validation.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4f32fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7316da",
   "metadata": {},
   "source": [
    "The first model I want to try is a simple logistic regression model.\n",
    "Lets start with a simple test model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f3241d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Variable Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc073873",
   "metadata": {},
   "source": [
    "One question here is how to choose a proper encoding for some of the variables.\n",
    "\n",
    "1. Hour of the day: One approach is convert it into a cyclic variable, and another is to use fixed \"binned\" time intervales like morning, midday, evening. We could also convert each hour into a category, but this does not make much sense.\n",
    "2. Numerical Features should be scaled to be comparable? -> Only relevant for gradient optimization, but might want to scale to 0 mean? Also affects regularization, which is on by default -> SHOULD STANDARDIZE, YES (also mentioned in elements of statistical learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f7eb5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hour_to_coordinate(h):\n",
    "    xh = np.sin(2*np.pi*(h)/24)\n",
    "    yh = np.cos(2*np.pi*(h)/24)\n",
    "    return xh,yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad66cc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_tod(data,dummies=False):\n",
    "    #Categories based on plots in Analysis Notebook\n",
    "    hours = data[\"starttime\"].dt.hour \n",
    "    bins=[-1,6,10,15,24] \n",
    "    names=[0,1,2,3]\n",
    "    tod = pd.cut(hours,bins,labels=names)\n",
    "    tod = tod.astype(\"int64\")\n",
    "    if dummies:\n",
    "        tod = pd.get_dummies(tod,prefix=\"tod\",drop_first=True)\n",
    "    return tod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2064a1a-a38e-43b4-a1bd-c831f4ecd9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_encoding_by_usertype(column,data): #Encode by the frequency of customers and subscribers\n",
    "    counts = data.groupby(\"usertype\")[column].value_counts()\n",
    "    counts = counts / counts.groupby(\"usertype\").sum()\n",
    "    Customer_count = data[column].map(counts[\"Customer\"]).fillna(0).astype(float)\n",
    "    Subscriber_count = data[column].map(counts[\"Subscriber\"]).fillna(0).astype(float)\n",
    "    return Customer_count, Subscriber_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f92edabc-58ef-4485-8aee-521dc9f012d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_encode_stations(X,data):\n",
    "    C,S = frequency_encoding_by_usertype(\"start station id\",data)\n",
    "    X[\"start customer freq\"] = C\n",
    "    X[\"start subscriber freq\"]= S\n",
    "    C,S = frequency_encoding_by_usertype(\"start station id\",data)\n",
    "    X[\"stop customer freq\"] = C\n",
    "    X[\"stop subscriber freq\"]= S\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc84df6e-e5de-4375-9e91-d37d4f1f5578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_coefficients(model,X):\n",
    "    coefficients = pd.Series([model.intercept_[0]]+list(model.coef_[0]),index=[\"intercept\"]+list(X.columns))\n",
    "    print(\"Coefficients: \")\n",
    "    print(coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e13a0272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,X,Y,verbose=True):\n",
    "    #print some summary statistics about the model\n",
    "    #TODO: Add uncertainty estimates about these\n",
    "    Y_pred = model.predict(X)\n",
    "    acc = accuracy_score(Y,Y_pred)\n",
    "    confusion= confusion_matrix(Y,Y_pred,normalize=\"true\")\n",
    "    if verbose:\n",
    "        print(f\"Accuracy: {acc}\")\n",
    "        print(f\"Confusion: \")\n",
    "        print(confusion)\n",
    "    return acc,confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236d95ff",
   "metadata": {},
   "source": [
    "Note that we cannot encode the station id as a one-hot vector because it takes too much memory.\n",
    "Instead I will just encode whether the station is one of the top customer stations or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1c14e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_stations(data,k=20):\n",
    "    grouped = data_train.groupby(\"usertype\")[\"start station id\"].value_counts()\n",
    "    return grouped[\"Customer\"][0:k].index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7a962",
   "metadata": {},
   "source": [
    "TODO: Proper data preparation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3289b1ba",
   "metadata": {},
   "source": [
    "TODO: Take log of some features that span multiple orders of magnitude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60a7d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Optimize by not copying any data\n",
    "def load_and_preprocess(path,scaler=None): #TODO: Use pipelines instead\n",
    "    data = pd.read_parquet(path,engine=\"pyarrow\")\n",
    "    numerical = [\"haversine distance\",\"tripduration\",\"speed\"] #features to be scaled in the end\n",
    "    features=[\"tripduration\", \"summer\",\"business day\", \"haversine distance\", \"roundtrip\", \"speed\"]\n",
    "    unused = [c for c in data.columns if c not in features]\n",
    "    label=\"usertype\"\n",
    "    X = data.drop(columns=unused)\n",
    "    Y = data[label].copy()\n",
    "    \n",
    "    #TOD and interaction terms\n",
    "    tod=categorize_tod(data,dummies=True)\n",
    "    X = pd.concat([X,tod],axis=1,copy=False)\n",
    "    interaction = tod.mul(X[\"business day\"],axis=0)\n",
    "    interaction.columns = [\"business x \" + c for c in tod.columns]\n",
    "    X = pd.concat([X,interaction],axis=1,copy=False)\n",
    "    \n",
    "    #Try encoding start station as categorical\n",
    "    #topstations = top_stations(data,k=20)\n",
    "    #X[\"topstation\"] = data[\"start station id\"].isin(topstations)\n",
    "    \n",
    "    #Try encoding station by customer count\n",
    "    X = frequency_encode_stations(X,data)\n",
    "    \n",
    "    #Gender\n",
    "    #dum = pd.get_dummies(data,columns=[\"gender\"],drop_first=True,prefix=\"gender\")\n",
    "    \n",
    "    Y=(Y==\"Customer\")\n",
    "    if not scaler:\n",
    "        scaler = sklearn.preprocessing.MinMaxScaler() #MinMaxScaler or StandardScaler does not seem to matter. MinMaxScaler has advantage of preserving speed = 0 values for roundtrips\n",
    "        scaler = scaler.fit(X[numerical])\n",
    "    X[numerical] = scaler.transform(X[numerical])  \n",
    "    return X,Y,scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02de4067-5528-4bc9-846e-ac81f10aaf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(datapath):\n",
    "    X_train,Y_train,scaler = load_and_preprocess(path_train)\n",
    "    clf = LogisticRegression(max_iter=300) #Might make sense to use balanced class weights here\n",
    "    clf=clf.fit(X_train,Y_train)\n",
    "    evaluate_model(clf,X_train,Y_train)\n",
    "    return clf,scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2273d810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8905468036686415"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = DummyClassifier()\n",
    "baseline = baseline.fit(X_train,Y_train)\n",
    "acc_base = baseline.score(X_train,Y_train)\n",
    "acc_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35e03017",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=300) #Might make sense to use balanced class weights here\n",
    "clf=clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9406b880-01ae-42e3-a820-fbd6fabde0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train,scaler = load_and_preprocess(path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e466d87-4408-4d15-ad18-d734fd32e8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9387394851499196\n",
      "Confusion: \n",
      "[[0.98692923 0.01307077]\n",
      " [0.45334794 0.54665206]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(clf,X_train,Y_train)\n",
    "del X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb9c862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, Y_val,scaler = load_and_preprocess(path_val,scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "feeb2d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9389125762369973\n",
      "Confusion: \n",
      "[[0.98705856 0.01294144]\n",
      " [0.45305693 0.54694307]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9389125762369973,\n",
       " array([[0.98705856, 0.01294144],\n",
       "        [0.45305693, 0.54694307]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(clf,X_val,Y_val);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0941c80",
   "metadata": {},
   "source": [
    "Notes: By using gender=unknown, gender=male categorical features one can easily get about 94% training accuracy, and about 98% on subscribers. Using classes = balanced gives 79% accuracy overall, but like 82% on customers instead of 20% for unbalanced.\n",
    "\n",
    "Scaling data with min-max scaler seems to have no effect\n",
    "Using categorical tod encoding does not seem much different from using ordinal encoding or hours, at least if we dont use interactions.\n",
    "Adding interaction terms between tod and business day does not help much.\n",
    "\n",
    "Adding top_20_customer_start_station as label increases accuracy from about 89.5 % to 90%.\n",
    "Instead using customer counts of each start_station as a feature gives 90% training but only 89.5% validation accuracy\n",
    "\n",
    "Frequency encoding bot start and end station by both subscriber and customer also gives around 90% accuracy.\n",
    "\n",
    "Adding gender to this again gives about 94% accuracy, and 54% on customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a99ecf-2de7-4135-ae24-9b5bcd1ec624",
   "metadata": {},
   "source": [
    "Further ideas:\n",
    "1. add interaction term between start and end station\n",
    "2. different categories for tod and summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d4171b-d26b-49fd-9020-d92d60e5a9f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
